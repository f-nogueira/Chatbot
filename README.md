# ğŸŒ¸ Floricultura AI Chatbot (RAG + FastAPI + React)

Este projeto Ã© um assistente virtual inteligente desenvolvido como Case TÃ©cnico. O objetivo Ã© simular um atendimento de floricultura onde a InteligÃªncia Artificial responde dÃºvidas sobre o estoque utilizando dados reais do banco de dados (SQLite), evitando alucinaÃ§Ãµes atravÃ©s da tÃ©cnica **RAG (Retrieval-Augmented Generation)**.

## ğŸš€ Tecnologias Utilizadas

O projeto foi construÃ­do com uma arquitetura moderna e modular:

- **Frontend:** React + Vite + Axios (Interface SPA responsiva)
- **Backend:** FastAPI (API REST de alta performance)
- **Banco de Dados:** SQLite (PersistÃªncia de dados relacional)
- **IA / LLM:** Hugging Face Inference API (Modelos Open Source como Qwen/Mistral)
- **Arquitetura:** MVC Simplificado (SeparaÃ§Ã£o de Rotas, ServiÃ§os e Modelos)

## ğŸ“‚ Estrutura do Projeto

```text
chatbot/
â”œâ”€â”€ .env                 # VariÃ¡veis de ambiente (Token de SeguranÃ§a)
â”œâ”€â”€ database.py          # Gerenciamento de conexÃ£o com SQLite
â”œâ”€â”€ models.py            # Schemas Pydantic (ValidaÃ§Ã£o de dados)
â”œâ”€â”€ routes.py            # DefiniÃ§Ã£o dos Endpoints da API
â”œâ”€â”€ services.py          # LÃ³gica de NegÃ³cio (RAG + IntegraÃ§Ã£o com IA)
â”œâ”€â”€ main.py              # ConfiguraÃ§Ã£o inicial da aplicaÃ§Ã£o
â”œâ”€â”€ requirements.txt     # Lista de dependÃªncias Python
â””â”€â”€ frontend/            # AplicaÃ§Ã£o React (Interface)
```

## âš™ï¸ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

Siga os passos abaixo para rodar o projeto no seu ambiente local.

### 1. ConfiguraÃ§Ã£o do Backend (API)

Navegue atÃ© a pasta raiz do projeto:

```bash
# Cria o ambiente virtual
python -m venv venv

# Ativa o ambiente
# Windows:
.\venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Instala as dependÃªncias
pip install -r requirements.txt
```

ğŸ” **ConfiguraÃ§Ã£o de SeguranÃ§a (.env):**  
Crie um arquivo chamado `.env` na raiz do projeto e adicione seu token da Hugging Face:

```ini
HF_TOKEN=seu_token_aqui_sem_aspas
```

### 2. ConfiguraÃ§Ã£o do Frontend (Interface)

Abra um novo terminal e entre na pasta do frontend:

```bash
cd frontend

# Instala as dependÃªncias do Node.js
npm install
```

## â–¶ï¸ Como Rodar a AplicaÃ§Ã£o

Para o sistema funcionar, Ã© necessÃ¡rio manter dois terminais abertos simultaneamente.

**Terminal 1: Servidor Backend**

```bash
# Na raiz do projeto (com a venv ativada)
uvicorn main:app --reload
```
O servidor iniciarÃ¡ em: [http://127.0.0.1:8000](http://127.0.0.1:8000)  
DocumentaÃ§Ã£o automÃ¡tica (Swagger): [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

**Terminal 2: Cliente Frontend**

```bash
# Na pasta /frontend
npm run dev
```
O site estarÃ¡ acessÃ­vel no link mostrado (geralmente [http://localhost:5173](http://localhost:5173))

## ğŸ§ª Testes Automatizados

O projeto inclui testes para validar a conexÃ£o com o banco e a formataÃ§Ã£o dos dados para a IA.

```bash
# Na raiz do projeto
pytest
```

## ğŸ§  Fluxo da Arquitetura (RAG)

1. **InteraÃ§Ã£o:** O usuÃ¡rio envia uma pergunta via React (ex: "Tem rosas?").
2. **RecuperaÃ§Ã£o (Retrieval):** O Backend (`services.py`) consulta o banco SQL e transforma o estoque tabular em texto natural.
3. **ContextualizaÃ§Ã£o:** O sistema cria um prompt contendo o estoque real + a pergunta do usuÃ¡rio.
4. **GeraÃ§Ã£o (Generation):** A LLM processa o prompt e gera uma resposta humanizada, baseada estritamente nos dados fornecidos.
5. **Resposta:** O Frontend exibe a mensagem final ao usuÃ¡rio.

---

Desenvolvido por Felipe Lima para Processo Seletivo de Desenvolvedor Python + IA.
